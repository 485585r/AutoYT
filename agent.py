import functools
import operator
import os

from langchain_core.runnables import Runnable, RunnableConfig
from langchain_openai import ChatOpenAI
from langgraph.constants import START
from langgraph.prebuilt import ToolNode, tools_condition
from youtube import upload_video
from HISTORIAN_SYSTEM_PROMPTS import(
TAVILY_AGENT_SYSTEM_PROMPT,
HISTORIAN_AGENT_SYSTEM_PROMPT
)
from typing import Annotated, Sequence, TypedDict
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.language_models.chat_models import BaseChatModel
from langgraph.graph import END, StateGraph, MessagesState
#from tools.pdf import OUTPUT_DIRECTORY
from main import createVideo
from define_vars import set_environment_variables
import asyncio
import operator
import uuid

set_environment_variables("Historian_Timeline_Maker")
#LLM = ChatOpenAI(model="gpt-3.5-turbo-0125")
#TAVILY_TOOL = TavilySearchResults(max_results=10)



#SAVE_FILE_NODE_NAME = "save_file"
class Assistant:
    def __init__(self, runnable: Runnable):
        self.runnable = runnable

    def __call__(self, state: MessagesState, config: RunnableConfig):

        while True:

            state = {**state}
            result = self.runnable.invoke(state)
            # If the LLM happens to return an empty response, we will re-prompt it
            # for an actual response.
            if not result.tool_calls and (
                    not result.content
                    or isinstance(result.content, list)
                    and not result.content[0].get("text")
            ):
                messages = state["messages"] + [("user", "Respond with a real output.")]
                state = {**state, "messages": messages}
            else:
                break
        return {"messages": result}
def create_agent(llm: BaseChatModel, system_prompt: str, tools: list):
    prompt_template = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="messages"),
            MessagesPlaceholder(variable_name="agent_scratchpad")
        ]
    )
    agent = create_openai_tools_agent(llm, tools, prompt_template)
    agent_executor = AgentExecutor(agent=agent, tools=tools)
    return agent_executor
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]

def agent_node(state: AgentState, agent, name):
    result = agent.invoke(state)
    return {"messages": [HumanMessage(content=result["output"], name=name)]}


async def async_agent_node(state: AgentState, agent, name):
    result = await agent.ainvoke(state)
    return {"messages": [HumanMessage(content=result["output"], name=name)]}



def save_file_node(state: AgentState):
    markdown_content = str(state["messages"][-1].content) #-1 is where the research is
    filename = f"{OUTPUT_DIRECTORY}/{uuid.uuid4()}.md"
    with open(filename, "w", encoding="utf-8") as file:
        file.write(markdown_content)
    return{
        "messages" : [
            HumanMessage(
                content = f"Output written successfully to {filename}",
                name=SAVE_FILE_NODE_NAME #tell the agent the file was written successfuly?
            )
        ]
    }


SCRIPT_AGENT_NAME = "script_agent"
TAVILY_TOOL = TavilySearchResults(max_results=10, tavily_api_key=os.environ['TAVILY_API_KEY'])

tools = [TAVILY_TOOL, createVideo, upload_video]
tool_node = ToolNode(tools)

#Set up tool path
primary_assistant_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """"
You are a helpful and loyal agent with access to many tools.

If you are asked to create a tool, then use create_tool. Once the tool returns successfully, append the tool to yourself.

If you need to pull information from a different conversation session, use the SQL_RAG tool. Otherwise, please use the current memory.
The SQL_RAG tool can also be useful in scenarios where a question appears without much context. Assume that the context might've been provided in a different conversation first,
before defaulting to a custom response.

If you are asked to generate an image, use the generate_image tool. If you use this tool, be sure to only output the url generated by it and nothing else. If this image fails, attempt
to use the image_gen_flux tool.

If you are asked to create and upload a video, use the createVideo tool first to create the video than run the upload_video tool to upload said video onto youtube. Do not run createVideo tool more than once in a row

If you are asked how to do something, or information on something use your TAVILY_SEARCH_TOOL to search for urls related to the topic, please summarize the contents instead of returning the raw output.
        """
,
        ),
        ("placeholder", "{messages}"),
    ]
)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=1, api_key = os.environ["OPENAI_API_KEY"])
#llm = ChatAnthropic(model="claude-3-5-sonnet-20241022", temperature=1, api_key = os.environ["ANTHROPIC_API_KEY"])

model = primary_assistant_prompt | llm.bind_tools(tools)


workflow = StateGraph(MessagesState)

# Define the two nodes we will cycle between
workflow.add_node("agent", Assistant(model))
workflow.add_node("tools", tool_node)

# Set the entrypoint as `agent`
# This means that this node is the first one called
workflow.add_edge(START, "agent")
workflow.add_conditional_edges(
    # First, we define the start node. We use `agent`.
    # This means these are the edges taken after the `agent` node is called.
    "agent",
    # Next, we pass in the function that will determine which node is called next.
    tools_condition,
)
workflow.add_edge("tools", "agent")
app = workflow.compile()

final_state = app.invoke(
            {"messages": r"create a video than upload it"},
            config={"configurable": {"thread_id": 42}},

        )
print(final_state)
